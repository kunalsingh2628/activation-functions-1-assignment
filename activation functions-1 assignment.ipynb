{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f40ba-6cf3-4758-95ad-0e6115cf5dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f2b05c-8bd2-4088-a949-330c636d5ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "An activation function is a mathematical operation applied to the output of each neuron in a neural network. It introduces non-linearity to the network, allowing it to learn from complex patterns in the data. The activation function determines whether a neuron should be activated or not, influencing the information that passes through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf2219-7089-47f3-bc43-ae43b351f7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f2c7aa-02dc-4146-8506-dca4481ca11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Several activation functions are commonly used in neural networks. Some of them include:\n",
    "\n",
    "Sigmoid function: Maps input values to a range between 0 and 1.\n",
    "Hyperbolic tangent (tanh) function: Similar to the sigmoid but maps input values to a range between -1 and 1.\n",
    "Rectified Linear Unit (ReLU): Sets all negative values to zero and passes positive values unchanged.\n",
    "Leaky ReLU: Similar to ReLU but allows a small, positive gradient for negative inputs.\n",
    "Softmax: Converts a vector of values into a probability distribution.\n",
    "Exponential Linear Unit (ELU): A variant of ReLU that smoothens the transition for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82de5c-748f-4985-851a-0750589f4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb9c45f-bd5a-4f3b-a088-9886aa206df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Activation functions introduce non-linearities to the network, allowing it to approximate complex mappings between inputs and outputs. This non-linearity is crucial for the network's ability to learn and represent intricate patterns in data. Different activation functions have varying effects on the training process, convergence speed, and the ability of the model to generalize to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e966de22-a202-4536-9e7b-5d56d0ed89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc16ca21-4685-4593-8809-cd797b9a52c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "The sigmoid activation function, also known as the logistic function, maps any real-valued number to the range between 0 and 1. It has the formula:\n",
    "\n",
    "sigmoid\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "1\n",
    "+\n",
    "�\n",
    "−\n",
    "�\n",
    "sigmoid(x)= \n",
    "1+e \n",
    "−x\n",
    " \n",
    "1\n",
    "​\n",
    " \n",
    "\n",
    "Advantages:\n",
    "\n",
    "It squashes input values to a range suitable for binary classification problems.\n",
    "It has a smooth gradient, making it well-suited for gradient-based optimization algorithms.\n",
    "Disadvantages:\n",
    "\n",
    "It can suffer from the vanishing gradient problem, making training slow or causing convergence issues in deep networks.\n",
    "The output is not centered around zero, leading to difficulties in weight updates during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20664cc4-4bb2-4979-9881-ce9a089410e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153d0a67-a4a0-4684-81a3-44da99048e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "The Rectified Linear Unit (ReLU) activation function is defined as \n",
    "ReLU\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "max\n",
    "⁡\n",
    "(\n",
    "0\n",
    ",\n",
    "�\n",
    ")\n",
    "ReLU(x)=max(0,x). It replaces all negative values with zero and passes positive values unchanged. ReLU introduces non-linearity and is computationally efficient.\n",
    "\n",
    "Differences from the sigmoid function:\n",
    "\n",
    "ReLU is not bound to a specific output range, allowing it to learn more diverse representations.\n",
    "It helps mitigate the vanishing gradient problem better than sigmoid, leading to faster training in deep neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da45853-0ac9-429f-908c-531949ef55c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad1932-1dcc-48c5-a4c2-5f71382d01de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Using ReLU over sigmoid offers several advantages:\n",
    "\n",
    "Mitigation of Vanishing Gradient: ReLU helps address the vanishing gradient problem better than sigmoid, enabling more effective training of deep networks.\n",
    "\n",
    "Computational Efficiency: ReLU is computationally more efficient than sigmoid and tanh, as it involves simple thresholding and does not require complex mathematical operations.\n",
    "\n",
    "Sparse Activation: ReLU tends to activate only a subset of neurons, promoting sparsity in activations, which can be beneficial for memory efficiency and generalization.\n",
    "\n",
    "However, it's important to note that ReLU is not without its challenges, such as the \"dying ReLU\" problem where neurons can become inactive during training and never recover. Variants like Leaky ReLU and Parametric ReLU aim to address this issue by allowing a small, non-zero gradient for negative inputs.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e5619-3f5f-4555-8ef4-775cfab8fa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f5b78b-1188-4242-95bd-0d53606a3491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caab5ba-cd04-4b26-936c-10da9f2005eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7c71b-55a7-4e75-a884-ccb4ff069cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd86f52a-1150-469e-86fc-e50388357519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a19c7-0bc2-4bda-8cf4-f07262e80b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c7f13e-5684-4700-93ab-340264deb842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e822203-8dab-4f12-a5e4-a0cf2cfcd015",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d6726-7f65-4d40-93cc-7d55e8181f7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d7ca4-632b-468b-ae33-54a2ee544a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d0cff4-4239-4811-a758-2b9e5fe2d45b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd390b8-df0e-4f60-9552-1a8e0dd9fe32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58a9a72-f385-4b50-9aac-0a35b2186d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ab2f2e-797e-42a1-a556-24ab970007e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61cce51-c243-4b44-8128-f17f1dc00031",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
